---
title: "Tidy indoctrination"
author: "Monica Thieu"
date: "July 13, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
```

## Goals for this vignette

* Demonstrate (what I think are) core features of the tidyverse
* Illustrate psychology use cases for tidyverse functions
* Hopefully convince you to join the cult of tidy

We shall do this by walking through a day in the life of a tidyverse user (me, I guess). This vignette will be set up in a sort of infomercial style, with a BEFORE TIDY (the grayscale part where the hapless folks in the infomercial cannot do a dang thing) and AFTER TIDY section (the nice color part where the gadget makes their life so much better).

My goal here is twofold:

* Map strategies that you already implement in a non-tidy way to their tidy counterparts
* Demonstrate possible pain points in non-tidy methods that are resolved in the tidyverse

### Please keep in mind!

This vignette is **not** intended to be an exhaustive walkthrough of all functions from the indicated `tidyverse` packages.

For reference, please see the excellent reference pages online for the relevant package. For a more detailed self-teaching experience, please refer to the incredible [R for Data Science online textbook](http://r4ds.had.co.nz/) by Garrett Grolemund and Hadley Wickham, which teaches using `tidyverse` functions.

## Quick reminder of key functions from various pkgs

Remember that each of these pkgs contain other functions too! Often these other functions are more specific use cases of the primary functions that crop up often enough that you'd want a separate function for them.

To see all the functions in each pkg, visit their reference pages at tidyverse.org ! I love these reference pages so much.

* `%>%`
* `tidyr`
    + `gather()` & `spread()` for making long and wide
    + `separate()` & `unite()` for fixing key columns (pre/post-processing for gather/spread)
    + `expand()`, `crossing()`, `nesting()`-- like `expand.grid()` but a little more predictable
    + `nest()`/`unnest()` for tibble-ception
* `dplyr`
    + `select()`, `filter()` for subsetting and reordering dfs/tibbles
    + `group_by()` for... grouping
    + `mutate()` for your column manipulation workhorse
    + `summarize()` for squishing multiple rows down to summary statistics
    + `arrange()` to sort dfs/tibbles more easily
* `purrr`
    + all the `map()` functions: like `apply()` but tidy
    
## Brief intro to the pipe

Before we begin...

Enter the pipe `%>%`! The pipe does one simple, but key, thing: **takes the object on the left-hand side and feeds it into the first argument of the function on the right-hand side.** This means that:

* `a %>% foo()` is equivalent to `foo(a)`. Fine and good
* `a %>% foo() %>% bar(arg = TRUE)` is equivalent to `bar(foo(a), arg = TRUE)`. Now, nested function calls read left-to-right!
* Most common use case: `df_new <- df_old %>% foo() %>% bar(arg = TRUE) %>% baz()` is equivalent to `df_new <- baz(bar(foo(df_old), arg = TRUE))`. Now, you can chain a series of preprocessing commands to operate on a dataframe all at once, and easily read those commands as typed in your script. No more accidentally not running some key preprocessing command that causes later code to break!

Note that #pipelife requires functions to:

* take as their first argument the object to be operated upon
* return the same object (or an analog of said), but now operated upon

Essentially all functions from the tidyverse are pipe-safe, but bear this in mind when trying to incorporate functions from base R or other packages into your tidy new world.

## Initializing fake data

We'll use a simulated dataset for this vignette, so you don't need to worry about any dependencies involving datasets you don't have access to while you're following along.

```{r}
# 20 fake subjects, 50 fake trials per subject

# Will simulate the person-level variables FIRST,
# then expand to simulate the trial-level variables
raw <- tibble(id = 1L:20L,
              age = sample(18L:35L, size = 20, replace = TRUE),
              # assuming binary gender for the purposes of this simulation
              gender = sample(c("male", "female"), size = 20, replace = TRUE)) %>%
  # simulating some "questionnaire" scores; person-level
  mutate(q_1 = rnorm(n = n(), mean = 30, sd = 10),
              q_2 = rnorm(n = n(), mean = 30, sd = 10),
              q_3 = rnorm(n = n(), mean = 30, sd = 10)) %>%
  # slice() subsets rows by position; you can use it to repeat rows by repeating position indices
  slice(rep(1:n(), each = 50)) %>%
  # We'll get to this in a bit--this causes every "group"
  # aka every set of rows with the same value for "id", to behave as an independent df
  group_by(id) %>%
  # I just want to have a column for "trial order", I like those in my task data
  mutate(trial_num = 1:n(),
         # Each subject sees half OLD and half NEW trials in this recognition memory task
         is_old = rep(0L:1L, times = n()/2),
         # I'm shuffling the order of "old" and "new" trials in my fake memory task
         is_old = sample(is_old),
         # This will generate binary "old"/"new" responses corresponding roughly to a d' of 1
         # yep, everyone has the same d' today
         response = if_else(is_old == 1,
                            rbinom(n = n(), size = 1, prob = 0.7),
                            rbinom(n = n(), size = 1, prob = 0.3)),
         rt = rnorm(n = n(), mean = 3, sd = 1.5)) %>%
  ungroup()
  
```


## Loading data

`readr` is great

### Non-tidy way

```{r}
wm_raw_untidy <- read.csv("~/Dropbox/MMT/data/MMT_filterObjs_all_final.csv", header = TRUE, stringsAsFactors = FALSE)

ltm_raw_untidy <- read.csv("~/Dropbox/MMT/data/MMT_testTargets_all_final.csv", header = TRUE, stringsAsFactors = FALSE)
```

### Tidy way

Don't forget to plug `read_csv_multi()` from this package

```{r, eval = FALSE}
data <- read_csv("/path/to/data.csv")
```

### Key differences

* in `readr`, header always `TRUE`, stringsAsFactors always `FALSE`
* `readr` always returns a tibble--good habits for later

### A taste of the possible

`googledrive` but it looks like you have to run through a temp mode of downloading the newest version from Google Drive and then reading that in from disk. OH but you could write a wrapper for this...

## Visually inspecting data

### Non-tidy way

To preview the data to see what it looks like briefly, `head()` is a common function that returns the first n rows/elements of a vector, dataframe, etc.

```{r}
head(wm_raw_untidy)
```

What happens if you forget to call `head()` and just print the whole dataframe?

```{r}
wm_raw_untidy
```

As Charles Barkley would say, turrible. Do you really need to look at the whole dataframe like this?

You may also step up one from `head()` and call `str()`:

```{r}
str(wm_raw_untidy)
```

Also, take a moment to talk about the Viewer--I myself used to be a stan for the Viewer but the key issue with the Viewer is that it becomes slow to load for large objects and is unwieldy for visual comparison of data in objects with very many rows and/or columns.

### Tidy way

`tibble` is LIFE

```{r}
raw
```

How does she print this way? She is a tibble and she is beautiful. Note that packages within the tidyverse work together--raw rectangular data read in using `readr` is given the tibble class by default.

```{r}
class(raw)
```

`"tbl_df"` is the class name for tibbles.

```{r}
class(wm_raw_untidy)
```

Dataframes can be coerced to tibble, though, if they have been created using base R.

```{r}
# This will give us the sweet sweet print method. It's like str() but better. And happens by default!
as_tibble(wm_raw_untidy)
```

### Key differences

Compared to a dataframe, a tibble:

* has a cleaner & more informative `print()` method
* is stricter when you create tibbles using `tibble()`, or when you index (with [] or $)

There are a couple other useful things that tibbles do that dataframes don't, but the strictness and the print method are the two most useful things that I find. As you'll see, imposing a bit more strictness on R can have the beneficial side effect of minimizing buggy cases in your own code, since you're forced to be more explicit.

## Subsetting data

We'll deal with subsetting dataframes two ways: choosing a subset of columns and a subset of rows.

### Subsetting by column

Note that these are **unquoted** variable names and not a character vector.

Also, just some nice shit that you can do with `select()` etc

```{r}
# This auto-indenting shit after the pipe works because the pipe is a left-right operator
# Renaming columns is so smooth in the tidyverse!
raw %>%
  select(id, age, gender, trial_num, is_old, response, rt)
```

```{r}
# Renaming columns is so smooth in the tidyverse!
raw %>%
  select(subject_id = id, age, gender, trial_num, is_old, response, rt)
```

```{r}
raw %>%
  select(id, starts_with("q"))
```


### Subsetting by row

#### Logical subsetting

Notice that we no longer have to index the original dataframe when specifying the logical vector(s) that will be used to subset rows

```{r}
raw %>%
  filter(id == 13)
```

#### Getting only unique/un-duplicated rows

Another function that technically "subsets" data, but not logically: `distinct()` is the `dplyr` function for removing rows with duplicate observations

```{r}
raw %>%
  select(id, gender, age) %>%
  # the .keep_all argument specifies whether to drop columns that aren't being distinct-ed by
  distinct(id, .keep_all = TRUE)
```

#### Subsetting random rows

This is something you might need to do on occasion. For example, sometimes when I want to plot single-subject data for just a few subjects from a dataset, I want to select a random subset of subjects so I'm not always just looking at the subjects who happened to be at the top of the df for whatever reason.

`dplyr` contains the functions `sample_n()` and `sample_frac()`, which wrap `base::sample()` to let you quickly get a random subset of rows, specified either by number of rows to get or by fraction of total rows to get.

Note that these work best if your df has one row per subject, because these don't select all the rows for a group.

### Subsetting by row and column simultaneously

In this case, order doesn't super matter, so it's a matter of personal preference. I would generally recommend putting all renaming calls as early as possible to allow the largest block of code to be consistent with itself.

```{r}
raw %>%
  select(id, age, gender, trial_num, is_old, response, rt) %>%
  filter(id == 13)
```

```{r}
# dplyr::filter() takes logical statements with & and | in them like you might use in bracket indexing,
# but it also has a nice feature: specifying separate logical statements as separate args,
# separate by commas, implicitly acts like chaining the statements together with &
# since the intersection of logical vectors is probably the most common use case for row subsetting
raw %>%
  select(id, age, gender, trial_num, is_old, response, rt) %>%
  filter(id == 13, is_old == 1)
```

### Key differences

Bracket indexing always happens from the global environment, while `dplyr` column/row subsetting verbs all act first within the local environment of the tibble of interest. For example, this means that when specifying columns for logical subsetting, columns can be named bare, without referencing the home tibble using $. This minimizes typing and also risk of accidentally referencing objects you didn't mean to reference.

## Manipulating data

`dplyr` goes here

### Creating/modifying columns

`dplyr::mutate()` will be your workhorse function for creating new columns or modifying existing ones in a df.

Inside of every `mutate()` call, as with previous `tidyverse` functions, column names are fed in as symbols (like variables, without quotes) without needing to $ index them from the parent df every time.

```{r}
processed <- raw %>%
  # select(-starts_with("q")) %>%
  mutate(rt_ms = round(rt * 1000))
```

You can do anything inside a `mutate()` call that:

* is totally element-wise
* returns a vector of length 1 (this will recycle)
* returns a vector of the same length as the total number of rows

Like `tibble()` for creating tibbles, `mutate()` will _not_ recycle vectors longer than 1 but shorter than the input df. I like this behavior, as it has protected me on many occasions from creating columns that didn't contain what I meant them to! If you do have a vector that you WANT recycled into a column, I would feed that vector into `rep()` so that you can explicitly create an output vector of the right length.

```{r}

```

```{r}
wm_tidy <- wm_raw_tidy %>%
  select(subjNum, groupStatus, numDistr = NumDistr, cond = Cond, resp, rt = RT) %>%
  # A tour of useful functions of sorts
  # For turning anything into a factor variable, special case of a switch statement where every case in the input has only one case in the output
  mutate(cond = recode_factor(cond,
                              `0` = "no_change",
                              `1` = "change"),
         # case_when: more general switch statement, can take any logical vector of appropriate length as a condition
         # remember that all tidyverse functions are evaluated sequentially, so these logical statements are hierarchical per an if-else if situation
         resp = case_when(resp == "j" ~ "no_change",
                          resp == "jj" ~ "no_change",
                          resp == "k" ~ "change",
                          resp == "kk" ~ "no_change",
                          TRUE ~ NA_character_),
         # nice function from the forcats pkg to reorder factor levels
         resp = fct_relevel(resp, c("no_change", "change")),
         # slightly more complex logical calls in case_when
         acc = case_when(cond == "change" & resp == "change" ~ "hit",
                         cond == "change" & resp == "no_change" ~ "miss",
                         cond == "no_change" & resp == "no_change" ~ "cr",
                         cond == "no_change" & resp == "change" ~ "fa",
                         TRUE ~ NA_character_),
         # coalesce is nice to fill all NAs in a vector with some other value
         acc_no_na = coalesce(acc, "no_response"),
         # conversely, should you need to replace a certain value in a vector with NA
         rt = na_if(rt, 0)) %>%
  # enter tidyverse GROUPING, which causes vectorized functions that would ordinarily operate over the whole length of the tibble to operate independently over GROUPS of the tibble defined by all visible levels of grouping variable
  group_by(groupStatus, subjNum, numDistr) %>%
  # one thing you can do with grouped tibbles is feed them directly into nest() and the grouping columns will become the label columns in the nested tibble
  # list-columns-of-tibbles!!!
  nest(.key = "data") %>%
  # this is the tibble-sorting function
  arrange(subjNum, numDistr) %>%
  # map()!!! we'll get into this
  # esp because summarize() is nested inside here
  mutate(overall = map(data, ~.x %>%
                         summarize(rate_hit = sum(acc_no_na == "hit") / sum(cond == "change"),
                                   rate_fa = sum(acc_no_na == "fa") / sum(cond == "no_change")) %>%
                         mutate(k = 2 * (rate_hit - rate_fa)))) %>%
  # to get out of list-column-of-tibbles format and back into one-long-tibble 
  unnest(overall, .preserve = "data")
```

#### Nice helpers

### Calculating summary statistics

```{r}
ltm_tidy <- ltm_raw_tidy %>%
  select(subjNum, groupStatus, numDistr = NumDistractorsStudy, oldNew = OldNew, resp = recogresp, rt = recogRT) %>%
  mutate(oldNew = recode_factor(oldNew,
                                New = "new",
                                Old = "old"),
         resp = case_when(resp == "j" ~ "old_high",
                          resp == "jj" ~ "old_high",
                          resp == "k" ~ "old_low",
                          resp == "kk" ~ "old_low",
                          resp == "f" ~ "new_high",
                          resp == "ff" ~ "new_high",
                          resp == "d" ~ "new_low",
                          resp == "dd" ~ "new_low",
                          TRUE ~ NA_character_)) %>%
  separate(resp, into = c("resp", "confidence")) %>%
  mutate(acc = case_when(oldNew == "old" & resp == "old" ~ "hit",
                         oldNew == "old" & resp == "new" ~ "miss",
                         oldNew == "new" & resp == "new" ~ "cr",
                         oldNew == "new" & resp == "old" ~ "fa",
                         TRUE ~ NA_character_),
         acc_no_na = coalesce(acc, "no_response"),
         rt = na_if(rt, 0)) %>%
  group_by(groupStatus, subjNum) %>%
  nest(.key = "data") %>%
  arrange(subjNum) %>%
  mutate(overall = map(data, ~.x %>%
                         summarize(rate_hit = sum(acc_no_na == "hit") / sum(oldNew == "old"),
                                   rate_fa = sum(acc_no_na == "fa") / sum(oldNew == "new")))) %>%
  unnest(overall, .preserve = "data")
# given that false alarms are all marked with numDistr == 0 may want to store this a slightly different way
```

Handily, `dplyr`'s general function for calculating summary statistics is `summarize()`! You can use this to generate any single summary value on a vector you can think of--mean, median, SD, you name it.

What constitutes a summary statistic? In this case, _any function that operates on a vector (or multiple vectors) and returns a single value (length 1) as output._ Gotta be length 1!

In `summarize()`, as in `mutate()`, the argument name will be the name of the new column generated by the code provided in the argument value.

(If you DON'T specify the column name (by leaving the argument name empty), the code you provided to calculate the column will automatically be substituted in as the name for the column. Don't do this...)

```{r}
# Just trying not to overload any function names
summaries <- processed %>%
  summarize(rt_mean = mean(rt),
            rt_median = median(rt))
```

#### Special versions of summarize()

`summarize()` lets you custom-specify every summary statistic you want to calculate, which is very nice indeed. Sometimes, however, you may have cases where you would like to calculate the same statistic(s) on very many variables, and you wouldn't like to type each of those out. We can handle this!

The two functions I use most often for this purpose are `summarize_if()` and `summarize_at()`.

`summarize_if()` lets you specify summary functions, and then applies those functions to _every column in your starting df that meets some logical criteria._ For example, let's say you want to calculate the mean and median of every column in a df that contains numeric data.

```{r}
# Creating one character column and three numeric columns so you can see that the character column shan't appear in the output of summarize_if()
tibble(chars = letters[1:10],
       nums1 = rnorm(10, mean = 0, sd = 1)) %>%
  mutate(nums2 = rnorm(n(), mean = 100, sd = 20),
         nums3 = rnorm(n(), mean = 0, sd = 20)) %>%
  summarize_if(is.numeric, funs(avg = mean(.),
                                median = median(.)))
```

The first true argument of `summarize_if()` is `.predicate`, where you specify the tester function that should return `TRUE` or `FALSE` for every **column** in your starter df.

The next argument is `.funs`, where you specify the summary function(s) you would like done to all of the columns for which `.predicate` returns `TRUE`. `dplyr` provides the helper function `funs()` to generate a list of function calls in the right format to be fed into `summarize_if()`. All you need is to specify the input function calls, each separated by a comma. The period **.** in the function calls represents "this df in which we are working". This allows you to tell `tidyverse` functions which argument in your summary functions will receive the data from the starter df that's being summarized. This is useful in cases where you have a summary function where the data isn't the first argument; this way you can make sure the data to be summarized doesn't get sent into the wrong argument and gums up the works.

You can see that `funs()` allows you to name the function calls you enter inside it, and that `summarize_if()` automatically appends the provided names of functions to the output cols.

The other specialty summarizing function, `summarize_at()`, does not use a logical predicate function to choose columns to summarize, but instead allows you to specify by name all the columns to summarize.

Instead of specifying a function in `.predicate`, that argument is replaced with `.vars`, where you specify the variables you want. You can do this a couple of ways:

With a character vector explicitly naming each column you want as a string:

```{r}
tibble(chars = letters[1:10],
       nums1 = rnorm(10, mean = 0, sd = 1)) %>%
  mutate(nums2 = rnorm(n(), mean = 100, sd = 20),
         nums3 = rnorm(n(), mean = 0, sd = 20)) %>%
  summarize_at(c("nums1", "nums2"), funs(avg = mean(.),
                                         median = median(.)))
```

With `tidyselect` helpers:

```{r}
tibble(chars = letters[1:10],
       nums1 = rnorm(10, mean = 0, sd = 1)) %>%
  mutate(nums2 = rnorm(n(), mean = 100, sd = 20),
         # This one doesn't start with "nums" like the other two do, notice!
         numbers3 = rnorm(n(), mean = 0, sd = 20)) %>%
  summarize_at(vars(starts_with("nums")), funs(avg = mean(.),
                                               median = median(.)))
```

Note that here, you have to wrap your `tidyselect` call (in this case `starts_with()`) inside a `vars()` call. `dplyr::vars()` is a helper function that lets you use `tidyselect` helper functions inside other `tidyverse` functions that aren't `select()`.

### Grouping

`dplyr` supports _grouping_ within dfs/tibbles. This means that, if groups are defined in your df, all operations that in some way operate along the nrows of your df now operate _independently for each group._ Your df still looks the same, but this grouping metadata will affect the behavior of some other `dplyr` functions called on your data.

```{r}
raw %>%
  group_by(id) %>%
  summarize(rt_median = median(rt))
```

You can group by as many variables as you would like. Each group will be composed of the observations belonging to one of the unique combinations of grouping variable levels present in your data.

### Creating new rows

#### Repeating rows

This strategy is inspired by a Stack Overflow post that I can't seem to find now, but I find this useful when I am generating simulated datasets with multilevel structure. Essentially, because `dplyr::slice()` and other numeric indexing methods (base R bracket indexing included) allow you to index the same position multiple times to get repeats in your output, you can repeat rows in your df by using `slice()` to index the same row multiple times.

If you want to repeat every row in your df, you can use the below syntax to `rep()` every index in your df (`1:n()`) `each` number of times. Specifying `each` instead of `times` ensures that all repeats of the same row stay next to each other, which is useful if you use this strategy to expand simulated person-level data to accommodate multiple observations per person.

```{r}
tibble(x = letters[1:5]) %>%
  slice(rep(1:n(), each = 3))
```


#### Completing combinations of grouping variables

There might be situations (particularly, those that are about to be fed into a `summarize()` call) where you want to complete combinations of grouping variables that might not exist in your data. For example, you might be working with a task where each trial can have a correct or incorrect response, and you'd like to use `summarize()` to count up the total number of correct and incorrect responses per person. If you have some star subject who gets all their trials correct, there will be no instances of that subject and the "incorrect" response value, so instead of showing a count of 0 rows, the summary will just be missing that row.

After you've called `summarize()`, you can use `tidyr::complete()` to create all the grouping variable combos you might have been missing (because they had no instances to count).

```{r}
tibble(x = letters[1:5]) %>%
  slice(rep(1:n(), each = 3)) %>%
  mutate(y = rbinom(n(), 1, 0.5)) %>%
  count(x, y) %>%
  # complete() must be called on ungrouped data, otherwise it attempts to complete the groups independently, which means it doesn't recognize when one group has levels of a variable that another group doesn't have
  ungroup() %>%
  complete(x, y, fill = list(n = 0L))
```

More broadly, you can use `tidyr` functions to do what you might do using `base::expand.grid()`, but with a bit more control. `tidyr::crossing()` is directly analogous to `base::expand.grid()`, in that it will create every possible combination of each of the provided variable levels.

```{r}
# in crossing(), vectors listed first will repeat slowest
crossing(x = letters[1:2], y = letters[3:4], z = 1:2)
```

## Reshaping data 

`tidyr` goes here, maybe. Since the example starts with trialwise data, maybe not yet? Should wait until we get to summary statistics?

### From long to wide



### From wide to long